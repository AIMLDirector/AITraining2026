{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e01e42-4b59-4a57-8a5b-ede358a82f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1659c3f0-a41f-400d-be70-787ffda29378",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem1 = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a4b8df0-75cf-47b4-8672-92c775b4f7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "stop_words = stopwords.words('english')\n",
    "sentence1 = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "word_sentence1 = word_tokenize(sentence1)\n",
    "print(word_sentence1)\n",
    "filtered_sentence1 = []\n",
    "\n",
    "for i in word_sentence1:\n",
    "    if i not in stop_words:\n",
    "        filtered_sentence1.append(i)\n",
    "\n",
    "print(filtered_sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db6d74fd-d384-4ec9-adaa-e35684db419c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'word', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "cleaned_token= []\n",
    "for w in filtered_sentence1:\n",
    "    i= lem1.lemmatize(w)\n",
    "    cleaned_token.append(i)\n",
    "    \n",
    "print(cleaned_token)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "652d7719-55bb-4e41-8d89-f7bffbd2b421",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_words = [\"word\", \"wordy\", \"wording\", \"cacti\", \"rocks\", \"catty\", \"demonic\", \"geese\", \"ravishing\", \"better\", \"best\", \"run\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a5452a2-9c2e-41f8-8aa5-4f9a47a4ff01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\n",
      "wordy\n",
      "wording\n",
      "cactus\n",
      "rock\n",
      "catty\n",
      "demonic\n",
      "goose\n",
      "ravishing\n",
      "better\n",
      "best\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "for w in example_words:\n",
    "    print(lem1.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "497a2666-99cc-44d7-ad19-c4af01a79dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "367a9fa3-b3ee-4872-99b3-7bca260ce8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'word', 'filtration', '.']\n",
      "[('This', 'DT'), ('sample', 'JJ'), ('sentence', 'NN'), (',', ','), ('showing', 'VBG'), ('stop', 'JJ'), ('word', 'NN'), ('filtration', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_token)\n",
    "pos_word = nltk.pos_tag(cleaned_token)\n",
    "print(pos_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cae626be-9c35-457b-ad30-85f4f179f1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tokens: expected a list of strings, got a string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m cleaned_token:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m----> 3\u001b[0m     pos_word \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mpos_tag(i)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(pos_word)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/nltk/tag/__init__.py:166\u001b[0m, in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03mUse NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03mtag the given list of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m:rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    165\u001b[0m tagger \u001b[38;5;241m=\u001b[39m _get_tagger(lang)\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/nltk/tag/__init__.py:120\u001b[0m, in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Throws Error if tokens is of string type\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tokens, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens: expected a list of strings, got a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     tagged_tokens \u001b[38;5;241m=\u001b[39m tagger\u001b[38;5;241m.\u001b[39mtag(tokens)\n",
      "\u001b[0;31mTypeError\u001b[0m: tokens: expected a list of strings, got a string"
     ]
    }
   ],
   "source": [
    "# for i in cleaned_token:\n",
    "#     print(i)\n",
    "#     pos_word = nltk.pos_tag(i)\n",
    "#     print(pos_word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973a2d27-a214-4c61-a8cc-99cb6500ac3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
